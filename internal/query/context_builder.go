package query

import (
	"encoding/binary"
	"encoding/json"
	"fmt"
	"math"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"unicode"

	"eulix/internal/config"
	"eulix/internal/embeddings"
	"eulix/internal/llm"
	"eulix/internal/types"
)

// These are written in bin files generated by eulix_embed
const (
	BinaryVersion = uint32(2)
	MagicBytes    = "EULX"
	VectorVersion = uint32(1)
)

func ContextWindowCreator(eulixDir string, cfg *config.Config, llmClient *llm.Client) (*ContextBuilder, error) {
	cb := &ContextBuilder{
		eulixDir:   eulixDir,
		config:     cfg,
		llmClient:  llmClient,
		vectorMap:  make(map[string]int),
	}

	// Initialize query embedder
	eulixBinaryPath := filepath.Join(eulixDir, "..", "eulix_embed")
	cb.queryEmbedder = embeddings.VectorWeaver(
		eulixBinaryPath,
		cfg.Embeddings.Model,
	)

	// Load pre-computed KB embeddings from embeddings.bin
	if err := cb.loadEmbeddings(); err != nil {
		cb.hasEmbeddings = false
	} else {
		cb.hasEmbeddings = true
	}

	// Load chunks from embeddings.json
	if err := cb.loadChunks(); err != nil {
		return nil, fmt.Errorf("failed to load chunks: %w", err)
	}

	// Load vector map from vectors.bin for fast ID lookups
	if err := cb.loadVectorMap(); err != nil {
		// Vector map is optional, continue without it
		cb.vectorMap = make(map[string]int)
	}

	// Try to load call graph
	cb.loadCallGraph()
	if err := cb.loadKnowledgeBase(); err != nil {
			cb.hasKB = false
		} else {
			cb.hasKB = true
		}

	return cb, nil
}

// Loaders for context
func (cb *ContextBuilder) loadKnowledgeBase() error {
	kbPath := filepath.Join(cb.eulixDir, "kb.json")
	data, err := os.ReadFile(kbPath)
	if err != nil {
		return fmt.Errorf("kb.json not found: %w", err)
	}

	var kb KnowledgeBase
	if err := json.Unmarshal(data, &kb); err != nil {
		return fmt.Errorf("failed to parse kb.json: %w", err)
	}

	cb.kbData = &kb
	return nil
}

func (cb *ContextBuilder) loadEmbeddings() error {
	embPath := filepath.Join(cb.eulixDir, "embeddings.bin")
	data, err := os.ReadFile(embPath)
	if err != nil {
		return fmt.Errorf("embeddings.bin not found: %w", err)
	}

	if len(data) < 16 { // Magic(4)+Version(4)+Num(4)+Dim(4)
		return fmt.Errorf("invalid embeddings file: too short (%d bytes)", len(data))
	}

	// Validate magic bytes
	magic := string(data[0:4])
	if magic != MagicBytes {
		return fmt.Errorf("invalid binary file: wrong magic bytes. Expected %q, got %q", MagicBytes, magic)
	}

	// Validate version
	version := binary.LittleEndian.Uint32(data[4:8])
	if version != BinaryVersion {
		return fmt.Errorf("version mismatch: expected %d, got %d", BinaryVersion, version)
	}

	// Parse counts
	numEmbeddings := binary.LittleEndian.Uint32(data[8:12])
	dimension := binary.LittleEndian.Uint32(data[12:16])

	if int(dimension) != cb.config.Embeddings.Dimension {
		return fmt.Errorf("dimension mismatch: expected %d, got %d", cb.config.Embeddings.Dimension, dimension)
	}

	// Validate file size
	expectedSize := 16 + (numEmbeddings * dimension * 4)
	if uint32(len(data)) != expectedSize {
		return fmt.Errorf("file size mismatch: expected %d bytes, got %d bytes", expectedSize, len(data))
	}

	cb.embeddings = make([][]float32, numEmbeddings)
	offset := 16

	for i := 0; i < int(numEmbeddings); i++ {
		embedding := make([]float32, dimension)
		for j := 0; j < int(dimension); j++ {
			if offset+4 > len(data) {
				return fmt.Errorf("unexpected EOF at embedding %d, dimension %d", i, j)
			}
			bits := binary.LittleEndian.Uint32(data[offset : offset+4])
			embedding[j] = math.Float32frombits(bits)
			offset += 4
		}
		cb.embeddings[i] = embedding
	}

	return nil
}

func (cb *ContextBuilder) loadChunks() error {
	embJsonPath := filepath.Join(cb.eulixDir, "embeddings.json")
	data, err := os.ReadFile(embJsonPath)
	if err != nil {
		return fmt.Errorf("failed to read embeddings.json: %w", err)
	}

	var embData EmbeddingsData
	if err := json.Unmarshal(data, &embData); err != nil {
		return fmt.Errorf("failed to parse embeddings.json: %w", err)
	}

	cb.embData = &embData
	cb.chunks = make([]Chunk, len(embData.Embeddings))

	for i, embChunk := range embData.Embeddings {
		symbols := extractSymbolsFromContent(embChunk.Content, embChunk.Metadata.Name)
		tokens := len(embChunk.Content) / 4

		cb.chunks[i] = Chunk{
			ID:        embChunk.ID,
			ChunkType: embChunk.ChunkType,
			File:      embChunk.Metadata.FilePath,
			StartLine: embChunk.Metadata.LineStart,
			EndLine:   embChunk.Metadata.LineEnd,
			Content:   embChunk.Content,
			Tokens:    tokens,
			Symbols:   symbols,
			Name:      embChunk.Metadata.Name,
			Importance: calculateImportance(embChunk.ChunkType, embChunk.Metadata.Complexity),
		}
	}

	return nil
}

// Load vector map from vectors.bin for fast ID-based lookups
func (cb *ContextBuilder) loadVectorMap() error {
	vectorsPath := filepath.Join(cb.eulixDir, "vectors.bin")
	data, err := os.ReadFile(vectorsPath)
	if err != nil {
		return fmt.Errorf("vectors.bin not found: %w", err)
	}

	if len(data) < 12 { // Version(4)+Count(8)
		return fmt.Errorf("invalid vectors file: too short (%d bytes)", len(data))
	}

	offset := 0

	// Read version
	version := binary.LittleEndian.Uint32(data[offset : offset+4])
	offset += 4
	if version != VectorVersion {
		return fmt.Errorf("vectors.bin version mismatch: expected %d, got %d", VectorVersion, version)
	}

	// Read count
	count := binary.LittleEndian.Uint64(data[offset : offset+8])
	offset += 8

	// Read dimension
	dimension := binary.LittleEndian.Uint32(data[offset : offset+4])
	offset += 4

	// Read each ID and build map
	for i := 0; i < int(count); i++ {
		// Read ID length
		if offset+4 > len(data) {
			return fmt.Errorf("unexpected EOF reading ID length at index %d", i)
		}
		idLen := binary.LittleEndian.Uint32(data[offset : offset+4])
		offset += 4

		// Read ID
		if offset+int(idLen) > len(data) {
			return fmt.Errorf("unexpected EOF reading ID at index %d", i)
		}
		id := string(data[offset : offset+int(idLen)])
		offset += int(idLen)

		// Store ID -> Index mapping
		cb.vectorMap[id] = i

		// Skip vector data
		offset += int(dimension) * 4
	}

	return nil
}

func (cb *ContextBuilder) loadCallGraph() {
	graphPath := filepath.Join(cb.eulixDir, "kb_call_graph.json")
	data, err := os.ReadFile(graphPath)
	if err != nil {
		cb.hasCallGraph = false
		return
	}

	var graphData CallGraphData
	if err := json.Unmarshal(data, &graphData); err != nil {
		cb.hasCallGraph = false
		return
	}

	cb.callGraph = make(map[string][]Relationship)
	for funcName, funcData := range graphData.Functions {
		relationships := make([]Relationship, 0)
		for _, callee := range funcData.Calls {
			relationships = append(relationships, Relationship{
				Type:     "calls",
				Target:   callee,
				Distance: 1,
			})
		}

		for _, caller := range funcData.CalledBy {
			relationships = append(relationships, Relationship{
				Type:     "called_by",
				Target:   caller,
				Distance: 1,
			})
		}

		cb.callGraph[funcName] = relationships
	}

	cb.hasCallGraph = len(cb.callGraph) > 0
}


// CORE LOGIC

// BuildContext is the key of the context window creation
func (cb *ContextBuilder) BuildContext(query string) (*types.ContextWindow, error) {
	systemPromptTokens := 150
	queryTokens := len(query) / 4
	safetyBuffer := 200
	responseReserve := 2000
	available := cb.config.LLM.MaxTokens - queryTokens - systemPromptTokens - safetyBuffer - responseReserve
	tokenBudget := int(float64(available) * 0.85)

	candidates := cb.multiStrategySearch(query, 100)

	var scored []ScoredChunk
	if cb.hasCallGraph {
		scored = cb.buildContextWithGraph(candidates, tokenBudget)
	} else {
		scored = cb.buildContextWithoutGraph(candidates, tokenBudget)
	}

	selected := cb.selectChunks(scored, tokenBudget)
	return cb.assembleContext(selected), nil
}

func (cb *ContextBuilder) buildContextWithGraph(candidates []ScoredChunk, budget int) []ScoredChunk {
	expanded := make(map[string]ScoredChunk)
	for _, c := range candidates {
		expanded[c.ID] = c
	}

	topN := 20
	if len(candidates) < topN {
		topN = len(candidates)
	}

	for i := 0; i < topN; i++ {
		candidate := candidates[i]
		for _, symbol := range candidate.Symbols {
			if rels, exists := cb.callGraph[symbol]; exists {
				for _, rel := range rels {
					score := candidate.Score
					if rel.Type == "calls" || rel.Type == "called_by" {
						score *= 0.9
						rel.Distance = 1
					} else if rel.Distance <= 2 {
						score *= 0.6
					} else {
						continue
					}

					for _, chunk := range cb.chunks {
						if contains(chunk.Symbols, rel.Target) {
							if existing, exists := expanded[chunk.ID]; !exists || score > existing.Score {
								expanded[chunk.ID] = ScoredChunk{
									Chunk:    chunk,
									Score:    score,
									Distance: rel.Distance,
									FromID:   candidate.ID,
								}
							}
							break
						}
					}
				}
			}
		}
	}

	result := make([]ScoredChunk, 0, len(expanded))
	for _, sc := range expanded {
		result = append(result, sc)
	}

	sort.Slice(result, func(i, j int) bool {
		return result[i].Score > result[j].Score
	})

	return result
}

func (cb *ContextBuilder) buildContextWithoutGraph(candidates []ScoredChunk, budget int) []ScoredChunk {
	if len(candidates) < 20 {
		return candidates
	}

	fileGroups := make(map[string][]ScoredChunk)
	for _, c := range candidates {
		fileGroups[c.File] = append(fileGroups[c.File], c)
	}

	type hotFile struct {
		file     string
		avgScore float64
		count    int
	}

	hotFiles := make([]hotFile, 0)
	for file, chunks := range fileGroups {
		if len(chunks) >= 3 {
			sum := 0.0
			for _, c := range chunks {
				sum += c.Score
			}
			hotFiles = append(hotFiles, hotFile{
				file:     file,
				avgScore: sum / float64(len(chunks)),
				count:    len(chunks),
			})
		}
	}

	sort.Slice(hotFiles, func(i, j int) bool {
		return hotFiles[i].avgScore > hotFiles[j].avgScore
	})

	hotFileMap := make(map[string]float64)
	for _, hf := range hotFiles {
		hotFileMap[hf.file] = hf.avgScore
	}

	for i := range candidates {
		if _, exists := hotFileMap[candidates[i].File]; exists {
			candidates[i].Score += 0.2
		}
	}

	sort.Slice(candidates, func(i, j int) bool {
		return candidates[i].Score > candidates[j].Score
	})

	return candidates
}

func (cb *ContextBuilder) assembleContext(chunks []Chunk) *types.ContextWindow {
	totalTokens := 0
	sources := make(map[string]bool)
	contextChunks := make([]types.ContextChunk, len(chunks))

	for i, chunk := range chunks {
		totalTokens += chunk.Tokens + 20
		sources[chunk.File] = true
		contextChunks[i] = types.ContextChunk{
			File:       chunk.File,
			StartLine:  chunk.StartLine,
			EndLine:    chunk.EndLine,
			Content:    chunk.Content,
			Importance: chunk.Importance,
		}
	}

	sourceList := make([]string, 0, len(sources))
	for source := range sources {
		sourceList = append(sourceList, source)
	}

	return &types.ContextWindow{
		Chunks:      contextChunks,
		TotalTokens: totalTokens,
		Sources:     sourceList,
	}
}

// Chunk and KB related Helpers

// KB-aware function/class lookup with full implementation details
func (cb *ContextBuilder) kbExactLookup(query string) []ScoredChunk {
	if !cb.hasKB {
		return nil
	}

	potentialSymbols := extractPotentialSymbols(query)
	scored := make([]ScoredChunk, 0)
	matchedIDs := make(map[string]bool)

	for _, symbol := range potentialSymbols {
		symbolLower := strings.ToLower(symbol)

		// Search in KB indices first (fastest)
		if locations, exists := cb.kbData.Indices.FunctionsByName[symbol]; exists {
			for _, loc := range locations {
				if !matchedIDs[loc] {
					matchedIDs[loc] = true
					if chunk := cb.findChunkForLocation(loc); chunk != nil {
						scored = append(scored, ScoredChunk{
							Chunk:        *chunk,
							Score:        120.0, // Highest priority
							Distance:     0,
							MatchType:    "kb_exact",
							MatchDetails: fmt.Sprintf("KB index: %s", symbol),
						})
					}
				}
			}
		}

		// Search through file structures
		for filePath, fileStruct := range cb.kbData.Structure {
			// Search functions
			for _, fn := range fileStruct.Functions {
				if strings.ToLower(fn.Name) == symbolLower {
					chunkID := fmt.Sprintf("%s:%d-%d", filePath, fn.LineStart, fn.LineEnd)
					if !matchedIDs[chunkID] {
						matchedIDs[chunkID] = true
						chunk := cb.buildChunkFromKBFunction(fn, filePath)
						scored = append(scored, ScoredChunk{
							Chunk:        chunk,
							Score:        115.0,
							Distance:     0,
							MatchType:    "kb_function",
							MatchDetails: fmt.Sprintf("Function: %s", fn.Name),
						})

						// Add related functions (calls and called_by)
						scored = append(scored, cb.expandFromKBFunction(fn, filePath, 110.0)...)
					}
				}
			}

			// Search classes and methods
			for _, class := range fileStruct.Classes {
				if strings.ToLower(class.Name) == symbolLower {
					chunkID := fmt.Sprintf("%s:%d-%d", filePath, class.LineStart, class.LineEnd)
					if !matchedIDs[chunkID] {
						matchedIDs[chunkID] = true
						chunk := cb.buildChunkFromKBClass(class, filePath)
						scored = append(scored, ScoredChunk{
							Chunk:        chunk,
							Score:        118.0,
							Distance:     0,
							MatchType:    "kb_class",
							MatchDetails: fmt.Sprintf("Class: %s", class.Name),
						})
					}
				}

				// Search methods within class
				for _, method := range class.Methods {
					if strings.ToLower(method.Name) == symbolLower {
						chunkID := fmt.Sprintf("%s:%d-%d", filePath, method.LineStart, method.LineEnd)
						if !matchedIDs[chunkID] {
							matchedIDs[chunkID] = true
							chunk := cb.buildChunkFromKBFunction(method, filePath)
							scored = append(scored, ScoredChunk{
								Chunk:        chunk,
								Score:        113.0,
								Distance:     0,
								MatchType:    "kb_method",
								MatchDetails: fmt.Sprintf("Method: %s.%s", class.Name, method.Name),
							})

							// Add parent class context
							scored = append(scored, ScoredChunk{
								Chunk:        cb.buildChunkFromKBClass(class, filePath),
								Score:        95.0,
								Distance:     1,
								MatchType:    "kb_parent_class",
								MatchDetails: fmt.Sprintf("Parent class of %s", method.Name),
							})
						}
					}
				}
			}
		}
	}

	return scored
}

func (cb *ContextBuilder) findChunkForLocation(location string) *Chunk {
	parts := strings.Split(location, ":")
	if len(parts) < 2 {
		return nil
	}

	file := parts[0]

	// try to parse line ranges
	var targetStart, targetEnd int
	if strings.Contains(parts[1], "-") {
		rangeParts := strings.Split(parts[1], "-")
		if len(rangeParts) == 2 {
			fmt.Sscanf(rangeParts[0], "%d", &targetStart)
			fmt.Sscanf(rangeParts[1], "%d", &targetEnd)

			// Find chunk that overlaps with target range
			for i := range cb.chunks {
				if cb.chunks[i].File == file {
					if cb.chunks[i].StartLine <= targetEnd && cb.chunks[i].EndLine >= targetStart {
						return &cb.chunks[i]
					}
				}
			}
		}
	} else {
		// Single line number
		var targetLine int
		fmt.Sscanf(parts[1], "%d", &targetLine)

		for i := range cb.chunks {
			if cb.chunks[i].File == file {
				if cb.chunks[i].StartLine <= targetLine && cb.chunks[i].EndLine >= targetLine {
					return &cb.chunks[i]
				}
			}
		}
	}

	return nil
}

func (cb *ContextBuilder) buildChunkFromKBFunction(fn KBFunction, filePath string) Chunk {
	content := fmt.Sprintf("Function: %s\nSignature: %s\nLines: %d-%d\n",
		fn.Name, fn.Signature, fn.LineStart, fn.LineEnd)

	if fn.Docstring != "" {
		content += fmt.Sprintf("Documentation: %s\n", fn.Docstring)
	}

	if len(fn.Calls) > 0 {
		content += "Calls:\n"
		for _, call := range fn.Calls {
			content += fmt.Sprintf("  - %s (line %d)\n", call.Callee, call.Line)
		}
	}

	return Chunk{
		ID:         fmt.Sprintf("%s:%d-%d", filePath, fn.LineStart, fn.LineEnd),
		ChunkType:  "function",
		File:       filePath,
		StartLine:  fn.LineStart,
		EndLine:    fn.LineEnd,
		Content:    content,
		Tokens:     len(content) / 4,
		Symbols:    []string{fn.Name},
		Name:       fn.Name,
		Importance: 0.9,
	}
}

func (cb *ContextBuilder) buildChunkFromKBClass(class KBClass, filePath string) Chunk {
	content := fmt.Sprintf("Class: %s\nLines: %d-%d\n", class.Name, class.LineStart, class.LineEnd)

	if class.Docstring != "" {
		content += fmt.Sprintf("Documentation: %s\n", class.Docstring)
	}

	if len(class.Methods) > 0 {
		content += "Methods:\n"
		for _, method := range class.Methods {
			content += fmt.Sprintf("  - %s (lines %d-%d)\n", method.Name, method.LineStart, method.LineEnd)
		}
	}

	symbols := []string{class.Name}
	for _, method := range class.Methods {
		symbols = append(symbols, method.Name)
	}

	return Chunk{
		ID:         fmt.Sprintf("%s:%d-%d", filePath, class.LineStart, class.LineEnd),
		ChunkType:  "class",
		File:       filePath,
		StartLine:  class.LineStart,
		EndLine:    class.LineEnd,
		Content:    content,
		Tokens:     len(content) / 4,
		Symbols:    symbols,
		Name:       class.Name,
		Importance: 0.95,
	}
}

func (cb *ContextBuilder) expandFromKBFunction(fn KBFunction, filePath string, baseScore float64) []ScoredChunk {
	expanded := make([]ScoredChunk, 0)

	// Add called functions
	for _, call := range fn.Calls {
		if call.DefinedIn != "" {
			// Find the called function in KB
			if fileStruct, exists := cb.kbData.Structure[call.DefinedIn]; exists {
				for _, calledFn := range fileStruct.Functions {
					if calledFn.Name == call.Callee {
						chunk := cb.buildChunkFromKBFunction(calledFn, call.DefinedIn)
						expanded = append(expanded, ScoredChunk{
							Chunk:        chunk,
							Score:        baseScore * 0.85,
							Distance:     1,
							MatchType:    "kb_called",
							MatchDetails: fmt.Sprintf("Called by %s", fn.Name),
						})
						break
					}
				}
			}
		}
	}

	return expanded
}

// Search Functions

//  Multi-strategy search with better identifier handling
func (cb *ContextBuilder) multiStrategySearch(query string, topK int) []ScoredChunk {
	allCandidates := make(map[string]ScoredChunk)

	// Strategy 0: KB exact lookup (HIGHEST PRIORITY)
	if cb.hasKB {
		kbMatches := cb.kbExactLookup(query)
		for _, match := range kbMatches {
			allCandidates[match.ID] = match
		}
	}

	// Strategy 1: Exact symbol match
	exactMatches := cb.exactSymbolSearch(query)
	for _, match := range exactMatches {
		if existing, exists := allCandidates[match.ID]; exists {
			match.Score = math.Max(existing.Score, match.Score) + 2.0
			match.MatchType = "kb+exact"
		} else {
			match.MatchType = "exact"
		}
		allCandidates[match.ID] = match
	}
	allCandidates = make(map[string]ScoredChunk)

	// Strategy 1: Exact symbol match (HIGHEST PRIORITY)
	exactMatches = cb.exactSymbolSearch(query)
	for _, match := range exactMatches {
		match.MatchType = "exact"
		allCandidates[match.ID] = match
	}

	// Strategy 2: Partial identifier match (split camelCase/snake_case)
	partialMatches := cb.partialIdentifierMatch(query)
	for _, match := range partialMatches {
		if existing, exists := allCandidates[match.ID]; exists {
			// Boost score if found by multiple strategies
			match.Score = math.Max(existing.Score, match.Score) + 1.5
			match.MatchType = "exact+partial"
		} else {
			match.MatchType = "partial"
		}
		allCandidates[match.ID] = match
	}

	// Strategy 3: Keyword search (HIGH PRIORITY)
	keywordMatches := cb.keywordSearch(query, topK)
	for _, match := range keywordMatches {
		if existing, exists := allCandidates[match.ID]; exists {
			// Boost score if found by multiple strategies
			match.Score = math.Max(existing.Score, match.Score) + 2.0
			match.MatchType = "exact+keyword"
		} else {
			match.MatchType = "keyword"
		}
		allCandidates[match.ID] = match
	}

	// Strategy 4: Semantic search (if embeddings available)
	if cb.hasEmbeddings {
		queryEmbedding, err := cb.queryEmbedder.EmbedQueryBinary(query)
		if err == nil {
			semanticMatches := cb.vectorSearch(queryEmbedding, topK, 0.5)
			for _, match := range semanticMatches {
				if existing, exists := allCandidates[match.ID]; exists {
					// Combine scores
					match.Score = existing.Score + match.Score*0.5
					if existing.MatchType == "exact" {
						match.MatchType = "exact+semantic"
					} else {
						match.MatchType = "keyword+semantic"
					}
				} else {
					match.MatchType = "semantic"
				}
				allCandidates[match.ID] = match
			}
		}
	}

	// Convert map to slice
	result := make([]ScoredChunk, 0, len(allCandidates))
	for _, chunk := range allCandidates {
		result = append(result, chunk)
	}

	// Sort by score (prioritize exact matches)
	sort.Slice(result, func(i, j int) bool {
		// Exact matches always come first
		if result[i].MatchType == "exact" && result[j].MatchType != "exact" {
			return true
		}
		if result[i].MatchType != "exact" && result[j].MatchType == "exact" {
			return false
		}
		return result[i].Score > result[j].Score
	})

	if len(result) > topK {
		result = result[:topK]
	}

	return result
}

// Exact symbol search for precise function/class lookups
func (cb *ContextBuilder) exactSymbolSearch(query string) []ScoredChunk {
	potentialSymbols := extractPotentialSymbols(query)
	scored := make([]ScoredChunk, 0)

	for _, chunk := range cb.chunks {
		nameLower := strings.ToLower(chunk.Name)

		// Check for exact name match
		for _, querySymbol := range potentialSymbols {
			querySymbolLower := strings.ToLower(querySymbol)
			if nameLower == querySymbolLower {
				scored = append(scored, ScoredChunk{
					Chunk:       chunk,
					Score:       100.0,
					Distance:    0,
					MatchDetails: fmt.Sprintf("Exact match: %s", chunk.Name),
				})
				break
			}
		}

		// Check symbols
		for _, symbol := range chunk.Symbols {
			symbolLower := strings.ToLower(symbol)
			for _, querySymbol := range potentialSymbols {
				querySymbolLower := strings.ToLower(querySymbol)
				if symbolLower == querySymbolLower {
					scored = append(scored, ScoredChunk{
						Chunk:       chunk,
						Score:       90.0,
						Distance:    0,
						MatchDetails: fmt.Sprintf("Symbol match: %s", symbol),
					})
					break
				}
			}
		}
	}

	return scored
}

// Match based on partial identifier components
func (cb *ContextBuilder) partialIdentifierMatch(query string) []ScoredChunk {
	queryTokens := extractPotentialSymbols(query)
	scored := make([]ScoredChunk, 0)
	matchedChunks := make(map[string]bool)

	for _, chunk := range cb.chunks {
		chunkNameTokens := splitIdentifierToTokens(chunk.Name)
		chunkNameLower := strings.ToLower(chunk.Name)

		matchCount := 0
		totalScore := 0.0
		matchedTokens := []string{}

		// Check if any query tokens match chunk name components
		for _, qToken := range queryTokens {
			qTokenLower := strings.ToLower(qToken)

			// Direct token match
			for _, cToken := range chunkNameTokens {
				if cToken == qTokenLower {
					matchCount++
					totalScore += 15.0
					matchedTokens = append(matchedTokens, cToken)
					break
				}
			}

			// Partial token match
			if strings.Contains(chunkNameLower, qTokenLower) && matchCount == 0 {
				totalScore += 8.0
				matchedTokens = append(matchedTokens, qTokenLower)
			}
		}

		// Also check symbols for partial matches
		for _, symbol := range chunk.Symbols {
			symbolTokens := splitIdentifierToTokens(symbol)
			symbolLower := strings.ToLower(symbol)

			for _, qToken := range queryTokens {
				qTokenLower := strings.ToLower(qToken)

				for _, sToken := range symbolTokens {
					if sToken == qTokenLower {
						matchCount++
						totalScore += 12.0
						matchedTokens = append(matchedTokens, sToken)
						break
					}
				}

				if strings.Contains(symbolLower, qTokenLower) && matchCount == 0 {
					totalScore += 6.0
					matchedTokens = append(matchedTokens, qTokenLower)
				}
			}
		}

		if matchCount >= 2 || (matchCount == 1 && totalScore > 15) {
			key := chunk.ID
			if !matchedChunks[key] {
				matchedChunks[key] = true
				scored = append(scored, ScoredChunk{
					Chunk:        chunk,
					Score:        totalScore,
					Distance:     0,
					MatchDetails: fmt.Sprintf("Partial match: %s", strings.Join(uniqueStrings(matchedTokens), ", ")),
				})
			}
		}
	}

	return scored
}

// keyword search
func (cb *ContextBuilder) keywordSearch(query string, topK int) []ScoredChunk {
	queryLower := strings.ToLower(query)
	keywords := extractQueryKeywords(queryLower)
	potentialSymbols := extractPotentialSymbols(query)
	scored := make([]ScoredChunk, 0)

	for _, chunk := range cb.chunks {
		score := 0.0
		contentLower := strings.ToLower(chunk.Content)
		nameLower := strings.ToLower(chunk.Name)

		matchDetails := []string{}

		// PRIORITY 1: Name matches
		for _, querySymbol := range potentialSymbols {
			querySymbolLower := strings.ToLower(querySymbol)
			if nameLower == querySymbolLower {
				score += 20.0
				matchDetails = append(matchDetails, fmt.Sprintf("name=%s", chunk.Name))
				break
			}

			if strings.Contains(nameLower, querySymbolLower) {
				score += 10.0
				matchDetails = append(matchDetails, fmt.Sprintf("name~%s", querySymbol))
			}
		}

		// PRIORITY 2: Symbol matches
		for _, symbol := range chunk.Symbols {
			symbolLower := strings.ToLower(symbol)
			for _, querySymbol := range potentialSymbols {
				querySymbolLower := strings.ToLower(querySymbol)
				if symbolLower == querySymbolLower {
					score += 15.0
					matchDetails = append(matchDetails, fmt.Sprintf("symbol=%s", symbol))
					break
				}

				if strings.Contains(symbolLower, querySymbolLower) {
					score += 7.0
				}
			}

			for _, keyword := range keywords {
				if symbolLower == keyword {
					score += 10.0
				} else if strings.Contains(symbolLower, keyword) {
					score += 5.0
				}
			}
		}

		// PRIORITY 3: Content keyword matches
		for _, keyword := range keywords {
			if strings.Contains(contentLower, keyword) {
				score += 2.0
				matchDetails = append(matchDetails, fmt.Sprintf("keyword=%s", keyword))
			}
		}

		// PRIORITY 4: File name relevance
		fileLower := strings.ToLower(chunk.File)
		for _, keyword := range keywords {
			if strings.Contains(fileLower, keyword) {
				score += 1.0
			}
		}

		// PRIORITY 5: Chunk type bonus
		switch chunk.ChunkType {
		case "function":
			score += 1.0
		case "class":
			score += 0.8
		case "method":
			score += 0.6
		}

		if score > 0 {
			scored = append(scored, ScoredChunk{
				Chunk:        chunk,
				Score:        score,
				Distance:     0,
				MatchDetails: strings.Join(matchDetails, ", "),
			})
		}
	}

	sort.Slice(scored, func(i, j int) bool {
		return scored[i].Score > scored[j].Score
	})

	if len(scored) > topK {
		scored = scored[:topK]
	}

	return scored
}

// Vector search using precomputed embeddings
func (cb *ContextBuilder) vectorSearch(queryEmb []float32, topK int, threshold float64) []ScoredChunk {
	scored := make([]ScoredChunk, 0)

	for i, chunkEmb := range cb.embeddings {
		if i >= len(cb.chunks) {
			break
		}

		similarity := cosineSimilarity(queryEmb, chunkEmb)
		if similarity >= threshold {
			scored = append(scored, ScoredChunk{
				Chunk:    cb.chunks[i],
				Score:    similarity,
				Distance: 0,
			})
		}
	}

	sort.Slice(scored, func(i, j int) bool {
		return scored[i].Score > scored[j].Score
	})

	if len(scored) > topK {
		scored = scored[:topK]
	}

	return scored
}


// Chunker Management Utilities

func (cb *ContextBuilder) selectChunks(scored []ScoredChunk, budget int) []Chunk {
	selected := make([]Chunk, 0)
	currentTokens := 0
	headerOverhead := 20
	coveredFiles := make(map[string]bool)

	for _, sc := range scored {
		chunkTokens := sc.Tokens + headerOverhead

		if currentTokens+chunkTokens > budget {
			break
		}

		if len(selected) > 0 {
			last := selected[len(selected)-1]
			if canMerge(last, sc.Chunk) {
				merged := mergeChunks(last, sc.Chunk)
				selected[len(selected)-1] = merged
				currentTokens += chunkTokens - headerOverhead
				continue
			}
		}

		selected = append(selected, sc.Chunk)
		currentTokens += chunkTokens
		coveredFiles[sc.File] = true
	}

	return selected
}

func (cb *ContextBuilder) Close() error {
	return nil
}

func extractSymbolsFromContent(content, name string) []string {
	symbols := []string{}
	if name != "" {
		symbols = append(symbols, name)
	}

	lines := strings.Split(content, "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "- ") {
			parts := strings.Split(line, " (")
			if len(parts) >= 1 {
				funcName := strings.TrimPrefix(parts[0], "- ")
				funcName = strings.TrimSpace(funcName)
				if funcName != "" && funcName != "..." {
					symbols = append(symbols, funcName)
				}
			}
		}
	}

	return symbols
}

func calculateImportance(chunkType string, complexity int) float64 {
	baseScore := 0.5
	switch chunkType {
	case "function":
		baseScore = 0.7
	case "class":
		baseScore = 0.8
	case "method":
		baseScore = 0.6
	case "file":
		baseScore = 0.4
	}

	if complexity > 5 {
		baseScore += 0.1
	}

	if complexity > 10 {
		baseScore += 0.1
	}

	if baseScore > 1.0 {
		baseScore = 1.0
	}

	return baseScore
}

func extractQueryKeywords(queryLower string) []string {
	stopWords := map[string]bool{
		"how": true, "does": true, "the": true, "a": true, "an": true,
		"is": true, "are": true, "what": true, "where": true, "when": true,
		"can": true, "will": true, "should": true, "would": true, "could": true,
		"this": true, "that": true, "these": true, "those": true, "of": true,
		"in": true, "on": true, "at": true, "to": true, "for": true, "with": true,
	}

	words := strings.FieldsFunc(queryLower, func(r rune) bool {
		return r == ' ' || r == ',' || r == '.' || r == '!' || r == '?' ||
			r == ';' || r == ':' || r == '(' || r == ')' || r == '[' || r == ']'
	})

	keywords := make([]string, 0)
	for _, word := range words {
		word = strings.Trim(word, "\"'")
		if len(word) > 2 && !stopWords[word] {
			keywords = append(keywords, word)
			if strings.Contains(word, "_") {
				parts := strings.Split(word, "_")
				for _, part := range parts {
					if len(part) > 2 && !stopWords[part] {
						keywords = append(keywords, part)
					}
				}
			}
		}
	}

	return keywords
}


// Helper Management Utilities

// Split identifiers into tokens for matching
func splitIdentifierToTokens(s string) []string {
	tokens := []string{}

	// Split snake_case by '_'
	for _, part := range strings.Split(s, "_") {
		// Split camelCase parts
		start := 0
		for i := 1; i < len(part); i++ {
			if unicode.IsUpper(rune(part[i])) {
				token := strings.ToLower(part[start:i])
				if token != "" && len(token) > 0 {
					tokens = append(tokens, token)
				}
				start = i
			}
		}
		if start < len(part) {
			token := strings.ToLower(part[start:])
			if token != "" && len(token) > 0 {
				tokens = append(tokens, token)
			}
		}
	}

	return tokens
}

//  Extract potential symbols from query with better tokenization
func extractPotentialSymbols(query string) []string {
	symbols := make([]string, 0)
	words := strings.Fields(query)

	for _, word := range words {
		word = strings.Trim(word, ".,!?;:'\"()[]{}")
		if len(word) > 2 {
			symbols = append(symbols, word)

			// Add split tokens for symbol parts (camelCase, snake_case)
			tokens := splitIdentifierToTokens(word)
			symbols = append(symbols, tokens...)
		}
	}

	return uniqueStrings(symbols)
}

func uniqueStrings(input []string) []string {
	seen := make(map[string]bool)
	result := make([]string, 0, len(input))
	for _, s := range input {
		if !seen[s] && s != "" {
			seen[s] = true
			result = append(result, s)
		}
	}
	return result
}

func cosineSimilarity(a, b []float32) float64 {
	if len(a) != len(b) {
		return 0.0
	}

	var dotProduct, normA, normB float64
	for i := range a {
		dotProduct += float64(a[i] * b[i])
		normA += float64(a[i] * a[i])
		normB += float64(b[i] * b[i])
	}

	if normA == 0 || normB == 0 {
		return 0.0
	}

	return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}

func contains(slice []string, item string) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

func canMerge(a, b Chunk) bool {
	if a.File != b.File {
		return false
	}

	gap := 0
	if a.EndLine < b.StartLine {
		gap = b.StartLine - a.EndLine
	} else if b.EndLine < a.StartLine {
		gap = a.StartLine - b.EndLine
	}

	return gap <= 5
}

func mergeChunks(a, b Chunk) Chunk {
	startLine := a.StartLine
	endLine := a.EndLine
	if b.StartLine < startLine {
		startLine = b.StartLine
	}

	if b.EndLine > endLine {
		endLine = b.EndLine
	}

	content := a.Content
	if b.StartLine > a.EndLine {
		content += "\n" + b.Content
	} else if a.StartLine > b.EndLine {
		content = b.Content + "\n" + content
	}

	symbols := make([]string, 0)
	symbolMap := make(map[string]bool)
	for _, s := range append(a.Symbols, b.Symbols...) {
		if !symbolMap[s] {
			symbols = append(symbols, s)
			symbolMap[s] = true
		}
	}

	return Chunk{
		ID:        a.ID,
		ChunkType: a.ChunkType,
		File:      a.File,
		StartLine: startLine,
		EndLine:   endLine,
		Content:   content,
		Tokens:    a.Tokens + b.Tokens,
		Symbols:   symbols,
		Name:      a.Name,
		Importance: math.Max(a.Importance, b.Importance),
	}
}
